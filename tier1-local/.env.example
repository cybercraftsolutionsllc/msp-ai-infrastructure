# --- Tier 1: Local Inference ---

# Versions
OPEN_WEBUI_VERSION=main
OLLAMA_VERSION=latest

# Ports
OPEN_WEBUI_PORT=3000
OLLAMA_PORT=11434

# Internal API Endpoints
# We use the service name 'ollama' because they are on the same network
OLLAMA_BASE_URL=http://ollama:11434

# General
RESTART_POLICY=unless-stopped
